This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-10-04 15:41:07

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
router_agent
  main.py
  README.md
mcp_news_aggr
  main.py
  fetch_news
    fetch_bbc_news.py
    fetch_all_news.py
    fetch_bloomberg_news.py
    fetch_google_news.py
    fetch_yle_news.py
    __init__.py
    fetch_forbes_news.py
    fetch_cnn_news.py
  mcp_server.py
  summarize_news.py
  __init__.py
mcp_prompt_opt
  overnight_opt.py
  _optimizer.py
  run_optimization.py
  mcp_test.py
  _prompt_factory.py
  mcp_server.py
  __init__.py
text_to_audio
  engines
    gcp.py
    main.py
  main.py
  gen.py
  README.md
mcp_humorizer
  humor.py
  mcp_server.py
  tests
    test_humor.py
  __init__.py
  mcp-humorizer.md
```

# Repository Files


## router_agent/main.py

```python
# In router_agent/main.py
import asyncio
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client


async def test_humorizer_http(text: str) -> str:
    """Call the dockerized MCP humorizer server"""
    async with streamablehttp_client("http://localhost") as (
        read_stream, write_stream, _
    ):
        async with ClientSession(read_stream, write_stream) as session:
            await session.initialize()
            tools = await session.list_tools()
            print(f"Tools: {tools}")

            result = await session.call_tool(
                "comedicize", {"id": "router-request", "summarized_text": text}
            )

            return result.content[0].text if result.content else text


async def main():
    text = "The economy shrank by 2% last quarter."
    # comedic_text = await test_humorizer_http(text)
    print("works") 
    print(f"Original: {text}")
    # print(f"Comedic: {comedic_text}")


if __name__ == "__main__":
    asyncio.run(main())
```

## router_agent/README.md

````markdown
# Development Workflow

This document outlines the development workflow for this Python 3.13 project.

## Prerequisites

-   **Python 3.13+** installed on your system
-   **Git** for version control

## Initial Setup

### 1. Create Virtual Environment

**Linux/macOS:**

```bash
python3 -m venv venv
```

**Windows:**

```cmd
python -m venv venv
```

> **Note:** Use `python3` on Linux/macOS and `python` on Windows (assuming Python 3.13 is your default Python).

### 2. Activate Virtual Environment

**Linux/macOS:**

```bash
source venv/bin/activate
```

**Windows (Command Prompt):**

```cmd
venv\Scripts\activate
```

**Windows (PowerShell):**

```powershell
venv\Scripts\Activate.ps1
```

> **Success indicator:** You should see `(venv)` at the beginning of your command prompt.

### 3. Install Dependencies

```bash
# Install development dependencies (includes production dependencies)
pip install -r requirements-dev.txt
```

## Daily Development Workflow

### Starting Work Session

**Linux/macOS:**

```bash
cd <to this root dir>
source venv/bin/activate
```

**Windows:**

```cmd
cd <to this root dir>
venv\Scripts\activate
```

### Running the Application

```bash
python3 main.py
```

### Ending Work Session

```bash
deactivate
```

## Managing Dependencies

### Adding New Dependencies

#### 1. Activate Virtual Environment First

**Linux/macOS:**

```bash
source venv/bin/activate  # if not active
```

**Windows:**

```cmd
venv\Scripts\activate  # if not active
```

#### 2. Install the Package

```bash
pip install package-name
```

#### 3. Add to Requirements File

**For production dependencies (needed to run the app):**

```bash
# Linux/macOS
echo "package-name>=version" >> requirements.txt

# Windows (Command Prompt)
echo package-name>=version >> requirements.txt

# Windows (PowerShell)
Add-Content requirements.txt "package-name>=version"
```

#### 4. Example: Adding Requests Library

**Complete workflow:**

```bash
# Activate environment
source venv/bin/activate  # Linux/macOS
# or venv\Scripts\activate  # Windows

# Install requests
pip install requests

# Check installed version
pip show requests

# Add to requirements (replace X.X.X with actual version)
echo "requests>=2.31.0" >> requirements.txt

# Verify it's added
cat requirements.txt  # Linux/macOS
# or type requirements.txt  # Windows
```

### Installing Dependencies from Requirements

When you pull changes that include new dependencies:

```bash
# Activate environment if not active
source venv/bin/activate  # Linux/macOS
# or venv\Scripts\activate  # Windows

# Update all dependencies
pip install -r requirements-dev.txt
```

### Viewing Installed Packages

```bash
pip list
```

### Removing a Package

```bash
pip uninstall package-name

# Don't forget to remove it from requirements.txt or requirements-dev.txt
```

## Quick Reference Commands

### Virtual Environment Management

| Action          | Linux/macOS                 | Windows                     |
| --------------- | --------------------------- | --------------------------- |
| Create venv     | `python3 -m venv venv`      | `python -m venv venv`       |
| Activate        | `source venv/bin/activate`  | `venv\Scripts\activate`     |
| Deactivate      | `deactivate`                | `deactivate`                |
| Check if active | Look for `(venv)` in prompt | Look for `(venv)` in prompt |

### Package Management

| Action                    | Command                               |
| ------------------------- | ------------------------------------- |
| Install package           | `pip install package-name`            |
| Install from requirements | `pip install -r requirements-dev.txt` |
| List packages             | `pip list`                            |
| Show package info         | `pip show package-name`               |
| Uninstall package         | `pip uninstall package-name`          |
| Upgrade package           | `pip install --upgrade package-name`  |

## Best Practices

1. **Always activate your virtual environment** before working on the project
2. **Update requirements files** immediately after installing new packages
3. **Use version pinning** (`package>=1.2.0`) in requirements files
4. **Never commit the `venv/` directory** to Git (it's in `.gitignore`)
5. **Deactivate** when switching to other projects
6. **Regular updates:** Periodically update your dependencies

## Quick Start Reminder

```bash
# Setup (once)
python3 -m venv venv                    # Linux/macOS
# or python -m venv venv                # Windows

# Daily workflow
source venv/bin/activate                # Linux/macOS
# or venv\Scripts\activate              # Windows
python -m src.main                      # Run app
deactivate                              # When done
```
````

## mcp_news_aggr/main.py

```python
from __future__ import annotations
import json
import os
import logging

try:
    from mcp_news_aggr.fetch_news.fetch_all_news import fetch_all_news
    from mcp_news_aggr.summarize_news import summarize_all_articles
except ModuleNotFoundError:
    from fetch_news.fetch_all_news import fetch_all_news
    from summarize_news import summarize_all_articles

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

JSON_FILE = "mcp_news_aggr/summarized_news.json"

def clear_json_file():
    """Reset the JSON file to an empty object before writing new data."""
    with open(JSON_FILE, "w", encoding="utf-8") as f:
        json.dump({}, f)

def main():
    # Always clear file at start

    articles = []
    try:
        articles = fetch_all_news(page_size=10, lang="en")
    except Exception as e:
        logger.exception("Error fetching news")

    if not articles:
        logger.error("No articles fetched.")
        return

    combined_texts = []
    for idx, article in enumerate(articles, start=1):
        combined_texts.append(
            f"Article {idx}:\n"
            f"Title: {article['title']}\n"
            f"Summary: {article['summary']}\n"
            f"Source: {article['source']}\n"
        )

    # Summarize
    full_summary = summarize_all_articles(combined_texts)

    clear_json_file()

    # Save JSON
    summarized_news = {
        "summary": full_summary
        #"articles": [{"title": a["title"], "source": a["source"], "url": a["url"]} for a in articles]
    }
    with open(JSON_FILE, "w", encoding="utf-8") as f:
        json.dump(summarized_news, f, ensure_ascii=False, indent=2)

    # Print summary
    print("=== Full Combined Summary ===")
    print(full_summary)


if __name__ == "__main__":
    main()


# Run with:
# python3 -m mcp_news_aggr.main
```

## mcp_news_aggr/fetch_news/fetch_bbc_news.py

```python
from GoogleNews import GoogleNews
from datetime import datetime, timedelta

def parse_date(date_str):
    today = datetime.today()
    date_str = date_str.lower()
    
    if "today" in date_str:
        return today.strftime("%Y-%m-%d")
    elif "yesterday" in date_str:
        return (today - timedelta(days=1)).strftime("%Y-%m-%d")
    elif "hours ago" in date_str or "minutes ago" in date_str:
        return today.strftime("%Y-%m-%d")
    else:
        try:
            parsed_date = datetime.strptime(date_str, "%b %d, %Y")
            return parsed_date.strftime("%Y-%m-%d")
        except:
            return today.strftime("%Y-%m-%d")

def fetch_bbc_news(page_size):
    googlenews = GoogleNews(lang='en', period='1d')
    #googlenews.set_topic("World News")
    googlenews.search("site:bbc.com")
    #googlenews.search("world news")
    #googlenews.search("breaking")

    for i in range(1,3):
        googlenews.get_page(i)
    
    #googlenews.get_page(1)

    results = googlenews.result()

    articles = []
    for item in results[:page_size]:
        title = item.get("title", "No title")
        summary = item.get("desc", "No summary")
        date_raw = item.get("date", "")
        date = parse_date(date_raw)
        url_link = item.get("link", "")
        source = item.get("media", "Google News")

        articles.append({
            "title": title,
            "summary": summary,
            "date": date,
            "url": url_link,
            "source": source
        })

    return articles
```

## mcp_news_aggr/fetch_news/fetch_all_news.py

```python
from mcp_news_aggr.fetch_news.fetch_google_news import fetch_google_news
from mcp_news_aggr.fetch_news.fetch_yle_news import fetch_yle_news
from mcp_news_aggr.fetch_news.fetch_bbc_news import fetch_bbc_news
from mcp_news_aggr.fetch_news.fetch_cnn_news import fetch_cnn_news
from mcp_news_aggr.fetch_news.fetch_bloomberg_news import fetch_bloomberg_news
from mcp_news_aggr.fetch_news.fetch_forbes_news import fetch_forbes_news

def fetch_all_news(page_size=10, lang="en"):
    """
    Fetch news from both Google News and Yle, combine them into one list.
    """
    google_articles = fetch_google_news(page_size=10)
    yle_articles = fetch_yle_news(page_size=5)
    bbc_articles = fetch_bbc_news(page_size=10)
    cnn_articles = fetch_cnn_news(page_size=10)
    bloomberg_articles = fetch_bloomberg_news(page_size=5)
    forbes_articles = fetch_forbes_news(page_size=5)
    

    all_articles = google_articles + yle_articles + bbc_articles + cnn_articles + bloomberg_articles + forbes_articles

    # Sort by date (newest first if possible)
    try:
        all_articles.sort(key=lambda x: x.get("date", ""), reverse=True)
    except Exception:
        pass

    return all_articles
```

## mcp_news_aggr/fetch_news/fetch_bloomberg_news.py

```python
from GoogleNews import GoogleNews
from datetime import datetime, timedelta

def parse_date(date_str):
    today = datetime.today()
    date_str = date_str.lower()
    
    if "today" in date_str:
        return today.strftime("%Y-%m-%d")
    elif "yesterday" in date_str:
        return (today - timedelta(days=1)).strftime("%Y-%m-%d")
    elif "hours ago" in date_str or "minutes ago" in date_str:
        return today.strftime("%Y-%m-%d")
    else:
        try:
            parsed_date = datetime.strptime(date_str, "%b %d, %Y")
            return parsed_date.strftime("%Y-%m-%d")
        except:
            return today.strftime("%Y-%m-%d")

def fetch_bloomberg_news(page_size):
    googlenews = GoogleNews(lang='en', period='1d')
    #googlenews.set_topic("World News")
    googlenews.search("site:bloomberg.com")
    #googlenews.search("world news")
    #googlenews.search("breaking")

    """for i in range(1,2):
        googlenews.get_page(i)"""
    
    googlenews.get_page(1)

    results = googlenews.result()

    articles = []
    for item in results[:page_size]:
        title = item.get("title", "No title")
        summary = item.get("desc", "No summary")
        date_raw = item.get("date", "")
        date = parse_date(date_raw)
        url_link = item.get("link", "")
        source = item.get("media", "Google News")

        articles.append({
            "title": title,
            "summary": summary,
            "date": date,
            "url": url_link,
            "source": source
        })

    return articles
```

## mcp_news_aggr/fetch_news/fetch_google_news.py

```python
from GoogleNews import GoogleNews
from datetime import datetime, timedelta

def parse_date(date_str):
    today = datetime.today()
    date_str = date_str.lower()
    
    if "today" in date_str:
        return today.strftime("%Y-%m-%d")
    elif "yesterday" in date_str:
        return (today - timedelta(days=1)).strftime("%Y-%m-%d")
    elif "hours ago" in date_str or "minutes ago" in date_str:
        return today.strftime("%Y-%m-%d")
    else:
        try:
            parsed_date = datetime.strptime(date_str, "%b %d, %Y")
            return parsed_date.strftime("%Y-%m-%d")
        except:
            return today.strftime("%Y-%m-%d")

def fetch_google_news(page_size):
    googlenews = GoogleNews(lang='en', period='1d')
    googlenews.set_topic("World News")
    googlenews.search("world news")
    #googlenews.search("world news")
    #googlenews.search("breaking")

    for i in range(1,3):
        googlenews.get_page(i)
    
    #googlenews.get_page(1)

    results = googlenews.result()

    articles = []
    for item in results[:page_size]:
        title = item.get("title", "No title")
        summary = item.get("desc", "No summary")
        date_raw = item.get("date", "")
        date = parse_date(date_raw)
        url_link = item.get("link", "")
        source = item.get("media", "Google News")

        articles.append({
            "title": title,
            "summary": summary,
            "date": date,
            "url": url_link,
            "source": source
        })

    return articles
```

## mcp_news_aggr/fetch_news/fetch_yle_news.py

```python
from GoogleNews import GoogleNews
from datetime import datetime, timedelta

def parse_date(date_str):
    today = datetime.today()
    date_str = date_str.lower()
    
    if "today" in date_str:
        return today.strftime("%Y-%m-%d")
    elif "yesterday" in date_str:
        return (today - timedelta(days=1)).strftime("%Y-%m-%d")
    elif "hours ago" in date_str or "minutes ago" in date_str:
        return today.strftime("%Y-%m-%d")
    else:
        try:
            parsed_date = datetime.strptime(date_str, "%b %d, %Y")
            return parsed_date.strftime("%Y-%m-%d")
        except:
            return today.strftime("%Y-%m-%d")

def fetch_yle_news(page_size):
    googlenews = GoogleNews(lang='en', period='1d')
    #googlenews.set_topic("World News")
    googlenews.search("site:yle.fi")
    #googlenews.search("world news")
    #googlenews.search("breaking")

    """for i in range(1,2):
        googlenews.get_page(i)"""
    
    googlenews.get_page(1)

    results = googlenews.result()

    articles = []
    for item in results[:page_size]:
        title = item.get("title", "No title")
        summary = item.get("desc", "No summary")
        date_raw = item.get("date", "")
        date = parse_date(date_raw)
        url_link = item.get("link", "")
        source = item.get("media", "Google News")

        articles.append({
            "title": title,
            "summary": summary,
            "date": date,
            "url": url_link,
            "source": source
        })

    return articles
```

## mcp_news_aggr/fetch_news/__init__.py

```python

```

## mcp_news_aggr/fetch_news/fetch_forbes_news.py

```python
from GoogleNews import GoogleNews
from datetime import datetime, timedelta

def parse_date(date_str):
    today = datetime.today()
    date_str = date_str.lower()
    
    if "today" in date_str:
        return today.strftime("%Y-%m-%d")
    elif "yesterday" in date_str:
        return (today - timedelta(days=1)).strftime("%Y-%m-%d")
    elif "hours ago" in date_str or "minutes ago" in date_str:
        return today.strftime("%Y-%m-%d")
    else:
        try:
            parsed_date = datetime.strptime(date_str, "%b %d, %Y")
            return parsed_date.strftime("%Y-%m-%d")
        except:
            return today.strftime("%Y-%m-%d")

def fetch_forbes_news(page_size):
    googlenews = GoogleNews(lang='en', period='1d')
    #googlenews.set_topic("World News")
    googlenews.search("site:forbes.com")
    #googlenews.search("world news")
    #googlenews.search("breaking")

    """for i in range(1,2):
        googlenews.get_page(i)"""
    
    googlenews.get_page(1)

    results = googlenews.result()

    articles = []
    for item in results[:page_size]:
        title = item.get("title", "No title")
        summary = item.get("desc", "No summary")
        date_raw = item.get("date", "")
        date = parse_date(date_raw)
        url_link = item.get("link", "")
        source = item.get("media", "Google News")

        articles.append({
            "title": title,
            "summary": summary,
            "date": date,
            "url": url_link,
            "source": source
        })

    return articles
```

## mcp_news_aggr/fetch_news/fetch_cnn_news.py

```python
from GoogleNews import GoogleNews
from datetime import datetime, timedelta

def parse_date(date_str):
    today = datetime.today()
    date_str = date_str.lower()
    
    if "today" in date_str:
        return today.strftime("%Y-%m-%d")
    elif "yesterday" in date_str:
        return (today - timedelta(days=1)).strftime("%Y-%m-%d")
    elif "hours ago" in date_str or "minutes ago" in date_str:
        return today.strftime("%Y-%m-%d")
    else:
        try:
            parsed_date = datetime.strptime(date_str, "%b %d, %Y")
            return parsed_date.strftime("%Y-%m-%d")
        except:
            return today.strftime("%Y-%m-%d")

def fetch_cnn_news(page_size):
    googlenews = GoogleNews(lang='en', period='1d')
    #googlenews.set_topic("World News")
    googlenews.search("site:cnn.com")
    #googlenews.search("world news")
    #googlenews.search("breaking")

    for i in range(1,3):
        googlenews.get_page(i)
    
    #googlenews.get_page(1)

    results = googlenews.result()

    articles = []
    for item in results[:page_size]:
        title = item.get("title", "No title")
        summary = item.get("desc", "No summary")
        date_raw = item.get("date", "")
        date = parse_date(date_raw)
        url_link = item.get("link", "")
        source = item.get("media", "Google News")

        articles.append({
            "title": title,
            "summary": summary,
            "date": date,
            "url": url_link,
            "source": source
        })

    return articles
```

## mcp_news_aggr/mcp_server.py

```python
from __future__ import annotations
import sys
import logging

try:
    from mcp.server.fastmcp import FastMCP
except Exception as e:
    print("ERROR: Missing or incompatible 'mcp' Python package. Install with:")
    print("  pip install -r requirements.txt")
    print(f"Details: {e}")
    sys.exit(1)

from mcp_news_aggr.fetch_news.fetch_all_news import fetch_all_news
from mcp_news_aggr.summarize_news import summarize_all_articles

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

app = FastMCP("mcp-news-aggr")

@app.tool()
def aggregate_news(page_size: int = 20, lang: str = "en") -> dict:
    try:
        articles = fetch_all_news(page_size=page_size, lang=lang)
        if not articles:
            return {"summary": "No articles found."}

        combined_texts = []
        for idx, article in enumerate(articles, start=1):
            combined_texts.append(
                f"Article {idx}:\nTitle: {article['title']}\n"
                f"Summary: {article['summary']}\nSource: {article['source']}"
            )

        summary = summarize_all_articles(combined_texts)
        return {
            "summary": summary,
            "articles": [{"title": a["title"], "source": a["source"], "url": a["url"]} for a in articles]
        }

    except Exception as e:
        logger.exception("Error in aggregate_news tool")
        return {"summary": f"Error fetching or summarizing news: {e}"}

@app.tool()
def health() -> dict:
    return {
        "name": "mcp-news-aggr",
        "status": "ok",
        "articles_provider": ["Google News", "Yle News"],
    }

def main() -> None:
    app.run()

if __name__ == "__main__":
    main()

# Run with:
# python3 -m mcp_news_aggr.mcp_server
```

## mcp_news_aggr/summarize_news.py

```python
import openai
#from config import OPENAI_API_KEY
from mcp_news_aggr.config import OPENAI_API_KEY

# Set OpenAI key
openai.api_key = OPENAI_API_KEY

def summarize_all_articles(articles):
    """
    Create a long, full-text summary of all articles combined.
    """
    if not articles:
        return "No articles available to summarize."

    combined_text = "\n\n---\n\n".join(articles)

    prompt = (
        "Act as a professional journalist. Write a detailed news digest summary. "
        "The summary should:\n"
        "- Cover all main events across the articles.\n"
        "- Highlight key players, locations, and timelines.\n"
        "- Explain the broader context and significance.\n"
        "- Do not skip any articles or information.\n"
        "- Be written in a clear, neutral, professional tone.\n\n"
        f"Articles:\n{combined_text}\n\n"
        "Now write the full summary:"
    )

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=2000
        )
        summary = response['choices'][0]['message']['content'].strip()
        return summary

    except openai.error.RateLimitError:
        return "Rate limit reached. Please try again later."

    except Exception as e:
        print("Error summarizing articles:", e)
        return "Error generating summary."
```

## mcp_news_aggr/__init__.py

```python

```

## mcp_prompt_opt/overnight_opt.py

```python
from __future__ import annotations
import os, json, asyncio
from dataclasses import asdict
from typing import List
from _optimizer import PromptPack, InputItem, tournament
from _prompt_factory import ask_prompt_generator, Request

TARGET_ELO = float(os.getenv("TARGET_ELO", "1500"))
TARGET_WINS = int(os.getenv("TARGET_WINS", "20"))
LIBRARY_PATH = os.getenv("LIBRARY_PATH", "variants.json")
LEADERBOARD_PATH = os.getenv("LEADERBOARD_PATH", "opt_logs/leaderboard_final.json")

import json, uuid
from typing import List, Dict, Any

def load_json_or_ndjson(path: str) -> List[Dict[str, Any]]:
    try:
        txt = open(path, "r", encoding="utf-8").read().strip()
    except FileNotFoundError:
        return []
    if not txt:
        return []

    try:
        obj = json.loads(txt)
        if isinstance(obj, list):
            return obj
        if isinstance(obj, dict):
            return [obj]
    except Exception:
        pass

    out: List[Dict[str, Any]] = []
    for ln in txt.splitlines():
        ln = ln.strip()
        if not ln:
            continue
        try:
            out.append(json.loads(ln))
        except Exception:
            # ignore bad lines
            continue
    return out

def ensure_prompt_pack_defaults(p: Dict[str, Any]) -> Dict[str, Any]:
    p = dict(p)
    p.setdefault("prompt_id", f"pp-{str(uuid.uuid4())[:8]}")
    p.setdefault("safety_profile", "standard")
    p.setdefault("style", "satirical")
    p.setdefault("angle", "Compare/Contrast")
    p.setdefault("structure", "Setup→Turn→Tag")
    p.setdefault("devices", ["Irony"])
    p.setdefault("word_cap", 60)
    p.setdefault("receipts_target", 2)
    p.setdefault("writer_system",
        "You are a concise comedy writer. Preserve supplied facts; do not invent. "
        "Think internally; do not show reasoning. Output 1–4 sentences, clear setup and punch. "
        "If facts conflict, switch to (parody)."
    )
    p.setdefault("writer_user_template",
        "PROMPT: {{prompt}}\nSUMMARY: {{summary}}\n"
        "TASK: Write a short caption (≤60 words) using Setup→Turn→Tag. "
        "Include up to 2 brief receipts only from SUMMARY. Output plain text only."
    )
    p.setdefault("few_shots", [])
    dp = p.get("decode_prefs") or {}
    dp.setdefault("temperature", 0.6)
    dp.setdefault("top_p", 0.9)
    p["decode_prefs"] = dp
    p["elo"] = float(p.get("elo") or 1000.0)
    p["wins"] = int(p.get("wins") or 0)
    p["losses"] = int(p.get("losses") or 0)
    return p


def _packs_from_dicts(items: List[dict]) -> List[PromptPack]:
    return [PromptPack(**ensure_prompt_pack_defaults(p)) for p in items]

async def bootstrap_if_needed() -> List[PromptPack]:
    items = load_json_or_ndjson(LIBRARY_PATH)
    if items:
        return _packs_from_dicts(items)
    req = Request(
        prompt="Make a witty short about elevator small talk.",
        summary="People feel awkward in elevators; ~30 seconds; silence vs forced chat; doors ding; everyone stares ahead.",
    )
    fresh = await ask_prompt_generator(req, n=12)
    packs = _packs_from_dicts(fresh)

    os.makedirs(os.path.dirname(LIBRARY_PATH) or ".", exist_ok=True)
    with open(LIBRARY_PATH, "w", encoding="utf-8") as f:
        f.write("\n".join(json.dumps(asdict(p), ensure_ascii=False) for p in packs))
    return packs

def _inputs() -> List[InputItem]:
    return [
        InputItem(
            prompt="Make a witty short about elevator small talk.",
            summary="People feel awkward in elevators; ~30 seconds; silence vs forced chat; doors ding; everyone stares ahead.",
        ),
        InputItem(
            prompt="Short caption on startup feature creep.",
            summary="Startup adds 5 new toggles to reduce confusion; users are more confused; PM writes a memo.",
        ),
        InputItem(
            prompt="Caption about online meeting chaos.",
            summary="Team joins call; half on mute; someone echoes; deck won’t load; meeting runs long.",
        ),
        InputItem(
            prompt="Joke about gym resolutions in January.",
            summary="Crowded gym in Jan; by March it’s empty except one guy filming squats.",
        ),
        InputItem(
            prompt="Short roast about corporate jargon.",
            summary="CEO email: 'synergy', 'leverage', 'circle back'; employees roll eyes.",
        ),
    ]

async def overnight_run():
    packs = await bootstrap_if_needed()
    inputs = _inputs()

    round_no = 0
    while True:
        round_no += 1
        print(f"=== Round {round_no} ===")

        packs = await tournament(
            packs=packs,
            inputs=inputs,
            iterations=3,
            samples_per_input=3,
            pairings=2,
            survivors=8,
            mutants_per_survivor=1,
            logdir="opt_logs",
        )

        os.makedirs(os.path.dirname(LIBRARY_PATH) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(LEADERBOARD_PATH) or ".", exist_ok=True)
        with open(LIBRARY_PATH, "w", encoding="utf-8") as f:
            json.dump([asdict(p) for p in packs], f, ensure_ascii=False, indent=2)
        with open(LEADERBOARD_PATH, "w", encoding="utf-8") as f:
            json.dump([asdict(p) for p in packs], f, ensure_ascii=False, indent=2)

        best = packs[0]
        print(f"Top: {best.prompt_id} elo={best.elo:.1f} wins={best.wins} losses={best.losses}")

        if best.elo >= TARGET_ELO and best.wins >= TARGET_WINS:
            print("🎉 Target reached. Stopping.")
            break

if __name__ == "__main__":
    asyncio.run(overnight_run())
```

## mcp_prompt_opt/_optimizer.py

```python
import re
import os, json, math, random, asyncio, uuid, time
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple, Optional
from mcp_prompt_opt._client import client as _client


PROMPT_JUDGE_SYSTEM_PROMPT = open("judge_prompt.txt").read()
MODEL = os.getenv("MODEL_NAME") or "gpt-4o-mini"
if not MODEL:
    raise ValueError("MODEL_NAME not set.")

@dataclass
class PromptPack:
    prompt_id: str
    safety_profile: str
    style: str
    angle: str
    structure: str
    devices: List[str]
    word_cap: int
    receipts_target: int
    writer_system: str
    writer_user_template: str
    few_shots: List[Dict[str, str]] | None = None
    decode_prefs: Dict[str, Any] | None = None
    audit: Dict[str, Any] | None = None
    eval_checks: List[str] | None = None
    elo: float = 1000.0
    wins: int = 0
    losses: int = 0


@dataclass
class InputItem:
    prompt: str
    summary: str


@dataclass
class MatchResult:
    a_id: str
    b_id: str
    winner_id: str
    confidence: float
    input_idx: int


@dataclass
class Generation:
    pack_id: str
    text: str
    meta: Dict[str, Any]


def fill_user_template(pack: PromptPack, item: InputItem) -> str:
    return pack.writer_user_template.replace("{{prompt}}", item.prompt).replace(
        "{{summary}}", item.summary
    )


def shortlist(packs: List[PromptPack], survivors: int) -> List[PromptPack]:
    return sorted(packs, key=lambda p: p.elo, reverse=True)[:survivors]


def k_factor(elo: float) -> float:
    if elo < 1200:
        return 40.0
    if elo < 1500:
        return 24.0
    if elo < 1800:
        return 16.0
    return 12.0


def elo_update(a: PromptPack, b: PromptPack, winner_id: str, conf: float = 0.7):
    if not a.elo:
        Ra = 1000.0
    else:
        Ra = a.elo

    if not b.elo:
        Rb = 1000.0
    else:
        Rb = b.elo

    if not b.wins:
        b.wins = 0

    if not b.losses:
        b.losses = 0

    if not a.wins:
        a.wins = 0

    if not a.losses:
        a.losses = 0

    Ea = 1.0 / (1.0 + 10 ** ((Rb - Ra) / 400))
    Eb = 1.0 / (1.0 + 10 ** ((Ra - Rb) / 400))
    Ka, Kb = k_factor(Ra), k_factor(Rb)
    Sa, Sb = (1.0, 0.0) if winner_id == a.prompt_id else (0.0, 1.0)
    w = max(0.0, min(1.0, (conf - 0.5) / 0.5))
    a.elo = Ra + Ka * w * (Sa - Ea)
    b.elo = Rb + Kb * w * (Sb - Eb)
    if winner_id == a.prompt_id:
        a.wins += 1
        b.losses += 1
    else:
        b.wins += 1
        a.losses += 1


def mutate(pack: PromptPack, p: float = 0.25) -> PromptPack:
    new = PromptPack(**asdict(pack))
    new.prompt_id = f"{pack.prompt_id}-m{str(uuid.uuid4())[:4]}"
    if random.random() < p and new.decode_prefs:
        new.decode_prefs["temperature"] = round(
            min(
                1.2,
                max(
                    0.2,
                    (new.decode_prefs.get("temperature"))
                    + random.choice([-0.2, -0.1, 0.1, 0.2]),
                ),
            ),
            2,
        )
    if random.random() < p:
        new.word_cap = random.choice([60, 140, new.word_cap])

    if random.random() < p:
        alt = [
            "Setup→Turn→Tag",
            "Rule of Three",
            "Angle–Example–Zinger",
            "Thesis→3 Receipts→Kicker",
        ]
        alt.remove(new.structure) if new.structure in alt else None
        new.structure = random.choice(alt)

    new.elo = max(
        900.0, new.elo - 50.0
    )

    new.wins = new.losses = 0
    return new


async def call_writer(pack: PromptPack, item: InputItem) -> Generation:
    user = fill_user_template(pack, item)
    print(user)
    messages = [{"role": "system", "content": pack.writer_system}]
    if pack.few_shots:
        for ex in pack.few_shots:
            messages.append({"role": "user", "content": f"SUMMARY: {ex['summary']}"})
            messages.append({"role": "assistant", "content": ex["output"]})

    messages.append({"role": "user",
        "content": user
        })

    dp = pack.decode_prefs or {}
    temperature = float(dp.get("temperature", 0.6))
    top_p = float(dp.get("top_p", 0.9))
    try:
        if _client:
            resp = await _client.chat.completions.create(
                model=MODEL,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=1200,
            )
            text = (resp.choices[0].message.content or "").strip()
        else:
            text = "Stub: elevator silence meets weather report. (parody)"

    except Exception as e:
        text = f"Stub due to error: {e}"

    return Generation(
        pack_id=pack.prompt_id,
        text=text,
        meta={"temperature": temperature, "top_p": top_p},
    )


async def call_judge(
    judge_system: str, 
    a_text: str,
    b_text: str, 
    summary: str
) -> Tuple[str, float]:
    user = (
        f"SUMMARY:\n{summary}\n\nA:\n{a_text}\n\nB:\n{b_text}\n\nReturn strictly JSON."
    )
    try:
        if _client:
            resp = await _client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": judge_system},
                    {"role": "user", "content": user},
                ],
                temperature=0.0,
                max_tokens=60,
                response_format={"type": "json_object"},
            )
            raw = (resp.choices[0].message.content or "").strip()
        else:
            raw = json.dumps(
                {
                    "winner": "A" if len(a_text) >= len(b_text) else "B",
                    "confidence": 0.6,
                }
            )

        obj = json.loads(raw)
        return obj.get("winner", "A"), float(obj.get("confidence", 0.6))
    except Exception:
        return ("A" if len(a_text) < len(b_text) else "B"), 0.55


async def tournament(
    packs: List[PromptPack],
    inputs: List[InputItem],
    iterations: int = 3,
    samples_per_input: int = 2,
    pairings: int = 1,
    survivors: int = 6,
    mutants_per_survivor: int = 1,
    logdir: str = "opt_logs",
):
    os.makedirs(logdir, exist_ok=True)
    judge_system = PROMPT_JUDGE_SYSTEM_PROMPT

    for it in range(iterations):
        round_log = []
        gens: Dict[Tuple[int, str], Generation] = {}
        for i, item in enumerate(inputs):
            chosen = random.sample(packs, min(samples_per_input, len(packs)))
            results = await asyncio.gather(*[call_writer(p, item) for p in chosen])
            for g in results:
                gens[(i, g.pack_id)] = g

        matches: List[MatchResult] = []
        for i, item in enumerate(inputs):
            cand = [p for p in packs if (i, p.prompt_id) in gens]
            if len(cand) < 2:
                continue
            for _ in range(pairings):
                a, b = random.sample(cand, 2)
                ga, gb = gens[(i, a.prompt_id)], gens[(i, b.prompt_id)]
                winner, conf = await call_judge(
                    judge_system, ga.text, gb.text, item.summary
                )
                win_id = a.prompt_id if winner == "A" else b.prompt_id
                matches.append(
                    MatchResult(
                        a_id=a.prompt_id,
                        b_id=b.prompt_id,
                        winner_id=win_id,
                        confidence=conf,
                        input_idx=i,
                    )
                )
                elo_update(a, b, win_id, conf)
                round_log.append(
                    {
                        "iter": it,
                        "input_idx": i,
                        "a": {"id": a.prompt_id, "elo": a.elo, "text": ga.text},
                        "b": {"id": b.prompt_id, "elo": b.elo, "text": gb.text},
                        "winner": winner,
                        "confidence": conf,
                    }
                )

        packs = shortlist(packs, survivors)
        new_mutants = []
        for p in packs:
            for _ in range(mutants_per_survivor):
                new_mutants.append(mutate(p))
        packs.extend(new_mutants)

        with open(os.path.join(logdir, f"round_{it}.jsonl"), "w") as f:
            for row in round_log:
                f.write(json.dumps(row, ensure_ascii=False) + "\n")
        with open(os.path.join(logdir, f"leaderboard_iter_{it}.json"), "w") as f:
            f.write(
                json.dumps(
                    [
                        asdict(p)
                        for p in sorted(packs, key=lambda x: x.elo, reverse=True)
                    ],
                    ensure_ascii=False,
                    indent=2,
                )
            )

    final = sorted(packs, key=lambda p: p.elo, reverse=True)
    with open(os.path.join(logdir, "leaderboard_final.json"), "w") as f:
        f.write(json.dumps([asdict(p) for p in final], ensure_ascii=False, indent=2))
    return final



def _coerce_pack_defaults(p: PromptPack) -> PromptPack:
    if p.elo is None or not isinstance(p.elo, (int, float)):
        p.elo = 1000.0
    if p.wins is None or not isinstance(p.wins, int):
        p.wins = 0
    if p.losses is None or not isinstance(p.losses, int):
        p.losses = 0
    if p.decode_prefs is None:
        p.decode_prefs = {"temperature": 0.6, "top_p": 0.9}
    else:
        p.decode_prefs.setdefault("temperature", 0.6)
        p.decode_prefs.setdefault("top_p", 0.9)
    return p



async def main():
    packs_raw = json.load(open("variants.json"))
    packs = [_coerce_pack_defaults(PromptPack(**p)) for p in packs_raw]

    inputs = [
        InputItem(
            prompt="Make a witty short about elevator small talk.",
            summary="People feel awkward in elevators; ~30 seconds; silence vs forced chat; doors ding; everyone stares ahead.",
        ),
        InputItem(
            prompt="Short caption on startup feature creep.",
            summary="Startup adds 5 new toggles to reduce confusion; users are more confused; PM writes a memo.",
        ),
    ]

    final = await tournament(
        packs=packs,
        inputs=inputs,
        iterations=3,
        samples_per_input=2,
        pairings=1,
        survivors=8,
        mutants_per_survivor=1,
        logdir="opt_logs",
    )

    print("Top 5:")
    for p in final[:5]:
        print(p.prompt_id, round(p.elo), p.style, p.structure)


if __name__ == "__main__":
    asyncio.run(main())
```

## mcp_prompt_opt/run_optimization.py

```python
import json, asyncio
from mcp_prompt_opt._optimizer import InputItem, PromptPack, tournament
from mcp_prompt_opt._prompt_factory import ask_prompt_generator, Request

async def main():
    req = Request(
        prompt="Make a witty short about elevator small talk.",
        summary="People feel awkward in elevators; 30 seconds; silence vs forced chat; doors ding; everyone stares ahead.",
    )

    packs = await ask_prompt_generator(req, n=24)
    with open("variants.json", "w") as f:
        f.write("\n".join(json.dumps(x, ensure_ascii=False) for x in packs))

    packs = [PromptPack(**p) for p in packs]
    inputs = [
        InputItem(
            prompt="Make a witty short about elevator small talk.",
            summary="People feel awkward in elevators; ~30 seconds; silence vs forced chat; doors ding; everyone stares ahead.",
        ),
        InputItem(
            prompt="Short caption on startup feature creep.",
            summary="Startup adds 5 new toggles to reduce confusion; users are more confused; PM writes a memo.",
        ),
        InputItem(
            prompt="Caption about online meeting chaos.",
            summary="Team joins video call; half are on mute; someone echoes; slide deck doesn’t load; meeting runs long.",
        ),
        InputItem(
            prompt="Joke about gym resolutions in January.",
            summary="Crowded gym in January; everyone signs up; by March, machines are empty except one guy filming squats.",
        ),
        InputItem(
            prompt="Short roast about corporate jargon.",
            summary="CEO email full of phrases like 'synergy', 'leverage', 'circle back'; employees roll eyes.",
        ),
        InputItem(
            prompt="Funny caption about procrastination.",
            summary="Deadline looms; person cleans desk, alphabetizes pens, watches 3 tutorials on productivity instead of working.",
        ),
        InputItem(
            prompt="Observational gag on airline boarding.",
            summary="Passengers crowd gate before boarding group called; overhead bins fill up; middle seat panic ensues.",
        ),
        InputItem(
            prompt="Satirical take on diet trends.",
            summary="New diet trend bans carbs, then bans fruit, then bans fun; influencer posts confusing meal plan.",
        ),
        InputItem(
            prompt="Witty remark on smart home devices.",
            summary="Smart fridge suggests expired kale smoothie; smart speaker misunderstands 'play jazz' as 'order 6 pizzas'.",
        ),
        InputItem(
            prompt="Comedic take on group projects.",
            summary="Group of 5 students; 1 does all the work; 2 argue about font size; 2 vanish until submission day.",
        ),
    ]

    final = await tournament(
        packs=packs,
        inputs=inputs,
        iterations=5,
        samples_per_input=3,
        pairings=2,
        survivors=8,
        mutants_per_survivor=1,
        logdir="opt_logs",
    )

    print("Top 5:")
    for p in final[:5]:
        print(p.prompt_id, round(p.elo), p.style, p.structure)


if __name__ == "__main__":
    asyncio.run(main())
```

## mcp_prompt_opt/mcp_test.py

```python
from mcp_server import health, optimize, best_prompt

print(health())

best = best_prompt(
    prompt="Make a witty short about elevators",
    summary="Awkward silence, 30s, doors ding",
    allow_quick_opt=True
)
print(best)

opt = optimize(
    prompt="Short caption on startup feature creep",
    summary="Startup adds 5 toggles to reduce confusion; users are more confused; PM writes memo.",
    n_new=8,       
    iterations=1,
    samples_per_input=2,
    pairings=1
)
print(opt)
```

## mcp_prompt_opt/_prompt_factory.py

````python
import os, json, uuid, asyncio
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv

load_dotenv()
from agents import Agent, OpenAIChatCompletionsModel, Runner

from mcp_prompt_opt._client import client

MODEL = os.getenv("MODEL_NAME") or "gpt-4o-mini"
if not MODEL:
    raise ValueError("MODEL_NAME not set.")

PROMPT_GENERATOR_SYSTEM_PROMPT = open("meta_system.txt").read()

@dataclass
class Request:
    prompt: str
    summary: str


REQUIRED_FIELDS = {
    "safety_profile", "style", "angle", "structure", "devices",
    "word_cap", "receipts_target", "writer_system", "writer_user_template"
}


def _ensure_id(obj: Dict[str, Any]) -> Dict[str, Any]:
    obj.setdefault("prompt_id", str(uuid.uuid4())[:8])
    return obj


def _valid_pack(obj: Dict[str, Any]) -> bool:
    return isinstance(obj, dict) and REQUIRED_FIELDS.issubset(set(obj.keys()))


def _fallback_pack() -> Dict[str, Any]:
    return {
        "prompt_id": "local-fallback",
        "safety_profile": "standard",
        "style": "satirical",
        "angle": "Compare/Contrast",
        "structure": "Setup→Turn→Tag",
        "devices": ["Irony"],
        "word_cap": 60,
        "receipts_target": 2,
        "writer_system": (
            "You are a concise comedy writer. Preserve supplied facts; do not invent. "
            "Think internally; do not show reasoning. Output 1–4 sentences, clear setup and punch. "
            "If facts conflict, switch to (parody)."
        ),
        "writer_user_template": (
            "PROMPT: {{prompt}}\nSUMMARY: {{summary}}\n"
            "TASK: Write a short caption (≤60 words) using Setup→Turn→Tag. "
            "Include up to 2 brief receipts only from SUMMARY. Output plain text only."
        ),
        "few_shots": [],
        "decode_prefs": {"temperature": 0.6, "top_p": 0.9, "presence_penalty": 0.0, "frequency_penalty": 0.2, "n": 1, "stop": []},
        "audit": {"mode": "real_news", "numbers_policy": "normalize; no invention", "attribution_policy": "omit unless asked", "failover": "contradictions → (parody)"},
        "eval_checks": ["Fidelity","Form","Safety","Device fit","Receipts"]
    }


async def _request_one_variant(agent: Agent, req: Request, max_tokens=1200, temperature=0.6, retries=2, idx: int = 0) -> Dict[str, Any]:
    """
    One independent Prompt-Generator call → ONE JSON object (a prompt pack).
    Retries with exponential backoff.
    """
    user_msg = (
        f"[PROMPT]\n{req.prompt}\n[/PROMPT]\n"
        f"[SUMMARY]\n{req.summary}\n[/SUMMARY]\n"
        f"Produce exactly 1 optimized prompt pack for the *Writer model*. "
        f"Return EXACTLY ONE JSON object (no extra text, no code fences). "
        f"The object MUST include fields: {sorted(REQUIRED_FIELDS)}."
    )
    messages = [
        {"role": "system", "content": PROMPT_GENERATOR_SYSTEM_PROMPT},
        {"role": "user", "content": user_msg},
    ]

    delay = 1.5
    for attempt in range(retries + 1):
        try:
            resp = await Runner.run(agent, messages)
            text = (getattr(resp, "final_output", "") or "").strip()
            if text.startswith("```"):
                first = text.find("{")
                last  = text.rfind("}")
                if first != -1 and last != -1:
                    text = text[first:last+1]

            obj = json.loads(text)
            if not isinstance(obj, dict):
                raise ValueError("Response is not a JSON object")
            if not _valid_pack(obj):
                raise ValueError(f"Missing required fields; got keys={list(obj.keys())}")
            return _ensure_id(obj)

        except Exception as e:
            if attempt >= retries:
                fb = _fallback_pack()
                fb["prompt_id"] = f"fallback-{idx}"
                return fb
            await asyncio.sleep(delay)
            delay *= 2


async def ask_prompt_generator(
    req: Request,
    n: int = 5,
    max_tokens: int = 1200,
    temperature: float = 0.6,
    retries: int = 2,
) -> List[dict]:
    """
    Launch N independent generation calls concurrently.
    """
    agent = Agent(
        name="prompt_generator",
        instructions=PROMPT_GENERATOR_SYSTEM_PROMPT,
        model=OpenAIChatCompletionsModel(model=MODEL, openai_client=client),
    )

    tasks = [
        _request_one_variant(agent, req, max_tokens=max_tokens, temperature=temperature, retries=retries, idx=i)
        for i in range(n)
    ]
    results = await asyncio.gather(*tasks)
    seen: set[Tuple] = set()
    uniq: List[Dict[str, Any]] = []
    for r in results:
        sig = (r.get("style"), r.get("angle"), r.get("structure"), r.get("word_cap"))
        if sig in seen:
            # tweak id to keep uniqueness visible
            r = dict(r)
            r["prompt_id"] = r["prompt_id"] + "-dup"
        else:
            seen.add(sig)
        uniq.append(r)
    return uniq


async def main():
    req = Request(
        prompt="Make a witty short about elevator small talk.",
        summary="People feel awkward in elevators; 30 seconds; silence vs forced chat; doors ding; everyone stares ahead.",
    )

    packs = await ask_prompt_generator(req, n=16)
    with open("variants.json", "w") as f:
        f.write("\n".join(json.dumps(x, ensure_ascii=False) for x in packs))

if __name__ == "__main__":
    asyncio.run(main())
````

## mcp_prompt_opt/mcp_server.py

```python
# mcp_prompt_opt/server.py
from __future__ import annotations
import os, json, asyncio, uuid
from typing import List, Dict, Any, Optional
from dataclasses import asdict
import logging

from mcp.server.fastmcp import FastMCP

from mcp_prompt_opt._prompt_factory import ask_prompt_generator, Request
from mcp_prompt_opt._optimizer import PromptPack, InputItem, tournament

logger = logging.getLogger("mcp-prompt-opt")
logging.basicConfig(level=logging.INFO)

app = FastMCP("mcp-prompt-opt")

LIBRARY_PATH = os.getenv("PROMPT_LIBRARY", "variants.json")
LEADERBOARD_PATH = os.getenv("PROMPT_LEADERBOARD", "opt_logs/leaderboard_final.json")
FAST_ITERATIONS = int(os.getenv("FAST_ITERATIONS", "1"))
FAST_SAMPLES = int(os.getenv("FAST_SAMPLES_PER_INPUT", "2"))
FAST_PAIRINGS = int(os.getenv("FAST_PAIRINGS", "1"))
FAST_SURVIVORS = int(os.getenv("FAST_SURVIVORS", "8"))
FAST_MUTANTS = int(os.getenv("FAST_MUTANTS_PER_SURVIVOR", "1"))


# ---------------- util ----------------
def _load_variants(path: str) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        return []

    txt = open(path).read().strip()
    if not txt:
        return []

    try:
        obj = json.loads(txt)
        if isinstance(obj, list):
            return obj
        if isinstance(obj, dict):
            return [obj]

    except Exception:
        pass
    out = []

    for line in txt.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            out.append(json.loads(line))
        except:
            continue
    return out


def _load_leaderboard(path: str) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        return []
    try:
        return json.load(open(path))
    except Exception:
        return []


def _packs_from_json(arr: List[Dict[str, Any]]) -> List[PromptPack]:
    def _coerce_num(x, default):
        try:
            return float(x) if isinstance(default, float) else int(x)
        except Exception:
            return default

    out: List[PromptPack] = []
    for p in arr:
        prompt_id = p.get("prompt_id") or f"pp-{str(uuid.uuid4())[:8]}"
        safety_profile = p.get("safety_profile") or "standard"
        style = p.get("style") or "satirical"
        angle = p.get("angle") or "Compare/Contrast"
        structure = p.get("structure") or "Setup→Turn→Tag"
        devices = p.get("devices") or ["Irony"]
        word_cap = int(p.get("word_cap") or 60)
        receipts_target = int(p.get("receipts_target") or 2)
        writer_system = p.get("writer_system") or (
            "You are a concise comedy writer. Preserve supplied facts; do not invent. "
            "Think internally; do not show reasoning. Output 1–4 sentences, clear setup and punch. "
            "If facts conflict, switch to (parody)."
        )
        writer_user_template = p.get("writer_user_template") or (
            "PROMPT: {{prompt}}\nSUMMARY: {{summary}}\n"
            "TASK: Write a short caption (≤60 words) using Setup→Turn→Tag. "
            "Include up to 2 brief receipts only from SUMMARY. Output plain text only."
        )

        few_shots = p.get("few_shots") or []
        decode_prefs = p.get("decode_prefs") or {}
        decode_prefs.setdefault("temperature", 0.6)
        decode_prefs.setdefault("top_p", 0.9)

        audit = p.get("audit") or {
            "mode": "real_news",
            "numbers_policy": "normalize; no invention",
            "attribution_policy": "omit unless asked",
            "failover": "contradictions → (parody)",
        }
        eval_checks = p.get("eval_checks") or ["Fidelity","Form","Safety","Device fit","Receipts"]

        elo = p.get("elo")
        wins = p.get("wins")
        losses = p.get("losses")
        elo = _coerce_num(elo, 1000.0) if elo is not None else 1000.0
        wins = _coerce_num(wins, 0) if wins is not None else 0
        losses = _coerce_num(losses, 0) if losses is not None else 0

        keep = {
            "prompt_id": prompt_id,
            "safety_profile": safety_profile,
            "style": style,
            "angle": angle,
            "structure": structure,
            "devices": devices,
            "word_cap": word_cap,
            "receipts_target": receipts_target,
            "writer_system": writer_system,
            "writer_user_template": writer_user_template,
            "few_shots": few_shots,
            "decode_prefs": decode_prefs,
            "audit": audit,
            "eval_checks": eval_checks,
            "elo": elo,
            "wins": wins,
            "losses": losses,
        }
        out.append(PromptPack(**keep))
    return out


def _best_ready(packs: List[PromptPack]) -> Optional[PromptPack]:
    candidates = [p for p in packs if (p.wins + p.losses) >= 12]
    if not candidates:
        candidates = packs
    if not candidates:
        return None
    return sorted(candidates, key=lambda x: (x.elo, x.wins - x.losses), reverse=True)[0]


def _default_inputs() -> List[InputItem]:
    return [
        InputItem(
            prompt="Make a witty short about elevator small talk.",
            summary="People feel awkward in elevators; ~30 seconds; silence vs forced chat; doors ding; everyone stares ahead.",
        ),
        InputItem(
            prompt="Short caption on startup feature creep.",
            summary="Startup adds 5 new toggles to reduce confusion; users are more confused; PM writes a memo.",
        ),
        InputItem(
            prompt="Caption about online meeting chaos.",
            summary="Team joins video call; half are on mute; someone echoes; slide deck doesn’t load; meeting runs long.",
        ),
        InputItem(
            prompt="Joke about gym resolutions in January.",
            summary="Crowded gym in January; everyone signs up; by March, machines are empty except one guy filming squats.",
        ),
        InputItem(
            prompt="Short roast about corporate jargon.",
            summary="CEO email full of phrases like 'synergy', 'leverage', 'circle back'; employees roll eyes.",
        ),
        InputItem(
            prompt="Funny caption about procrastination.",
            summary="Deadline looms; person cleans desk, alphabetizes pens, watches 3 tutorials on productivity instead of working.",
        ),
        InputItem(
            prompt="Observational gag on airline boarding.",
            summary="Passengers crowd gate before boarding group; overhead bins fill; middle seat panic ensues.",
        ),
        InputItem(
            prompt="Satirical take on diet trends.",
            summary="New diet bans carbs, then fruit, then fun; influencer posts confusing meal plan.",
        ),
        InputItem(
            prompt="Witty remark on smart home devices.",
            summary="Smart fridge suggests expired kale smoothie; speaker mishears 'play jazz' as 'order 6 pizzas'.",
        ),
        InputItem(
            prompt="Comedic take on group projects.",
            summary="Group of 5; 1 does all the work; 2 argue about font; 2 vanish until submission.",
        ),
    ]


async def _quick_opt(
    packs: List[PromptPack],
    inputs: List[InputItem],
    gen_more: bool,
    seed_prompt: str,
    seed_summary: str,
    n_new: int = 8,
) -> List[PromptPack]:
    """Optionally generate a few challengers, then run a short tournament."""
    base = [*packs]

    if gen_more:
        req = Request(prompt=seed_prompt, summary=seed_summary)
        new = await ask_prompt_generator(req, n=n_new)
        base.extend(_packs_from_json(new))

    final = await tournament(
        packs=base,
        inputs=inputs,
        iterations=FAST_ITERATIONS,
        samples_per_input=FAST_SAMPLES,
        pairings=FAST_PAIRINGS,
        survivors=FAST_SURVIVORS,
        mutants_per_survivor=FAST_MUTANTS,
        logdir="opt_logs",
    )

    return final


@app.tool()
def health() -> Dict[str, Any]:
    """Basic health + library stats."""
    lib = _packs_from_json(_load_variants(LIBRARY_PATH))
    lb = _packs_from_json(_load_leaderboard(LEADERBOARD_PATH))
    total = len(lib) or len(lb)
    return {"name": "mcp-prompt-opt", "library_prompts": total, "status": "ok"}


@app.tool()
def best_prompt(
    prompt: str, summary: str, allow_quick_opt: bool = True
) -> Dict[str, Any]:
    """
    Fast path: return the best available prompt pack for (prompt, summary).
    If allow_quick_opt=True and no confident winner exists, run a tiny optimize pass.
    Returns: {"prompt_pack": {...}, "note": str}
    """
    raw = _load_leaderboard(LEADERBOARD_PATH)
    if not raw:
        raw = _load_variants(LIBRARY_PATH)

    packs = _packs_from_json(raw)
    if not packs:
        return {
            "error": "No prompt library found. Generate variants first (variants.json or leaderboard)."
        }

    champ = _best_ready(packs)
    if champ and (champ.wins + champ.losses) >= 15 and champ.elo >= 1050:
        return {"prompt_pack": asdict(champ), "note": "selected_from_library"}

    if not allow_quick_opt:
        return {
            "prompt_pack": asdict(champ),
            "note": "selected_from_library_low_confidence",
        }

    inputs = _default_inputs()
    final_packs = asyncio.run(
        _quick_opt(
            packs,
            inputs,
            gen_more=True,
            seed_prompt=prompt,
            seed_summary=summary,
            n_new=6,
        )
    )
    best = final_packs[0]
    return {"prompt_pack": asdict(best), "note": "quick_optimized"}


@app.tool()
def optimize(
    prompt: str,
    summary: str,
    n_new: int = 12,
    iterations: int = 2,
    samples_per_input: int = 2,
    pairings: int = 1,
) -> Dict[str, Any]:
    """
    On-demand quick optimization for this (prompt, summary).
    Generates n_new challengers and runs a compact tournament over a small input set.
    Returns best prompt pack + lightweight leaderboard.
    """
    base_raw = _load_leaderboard(LEADERBOARD_PATH)
    if not base_raw:
        base_raw = _load_variants(LIBRARY_PATH)
    packs = _packs_from_json(base_raw)

    req = Request(prompt=prompt, summary=summary)
    new = asyncio.run(ask_prompt_generator(req, n=n_new))
    packs.extend(_packs_from_json(new))

    inputs = _default_inputs()
    final = asyncio.run(
        tournament(
            packs=packs,
            inputs=inputs,
            iterations=iterations,
            samples_per_input=samples_per_input,
            pairings=pairings,
            survivors=max(8, min(16, len(packs) // 2)),
            mutants_per_survivor=1,
            logdir="opt_logs",
        )
    )

    best = final[0]
    board = [
        {"prompt_id": p.prompt_id, "elo": p.elo, "wins": p.wins, "losses": p.losses}
        for p in final[:10]
    ]
    return {"best": asdict(best), "leaderboard_top10": board}


def main():
    app.run()


if __name__ == "__main__":
    main()
```

## mcp_prompt_opt/__init__.py

```python
__version__ = "0.1.0"
from .config import Settings
```

## text_to_audio/engines/gcp.py

```python
from engines import main
from google.cloud import texttospeech


class GoogleTextToSpeechClient(main.Engine):
    def __init__(self):
        self.gcp_tts_client = texttospeech.TextToSpeechClient()
        super(GoogleTextToSpeechClient, self).__init__()

    def synthesize(self, text_bits, voice="en-US-Chirp3-HD-Charon", lang_code="en-US"):
        for text in text_bits:
            try:
                input_text = texttospeech.SynthesisInput(text=text)
                voice = texttospeech.VoiceSelectionParams(
                    language_code=lang_code, name=voice
                )
                audio_config = texttospeech.AudioConfig(
                    audio_encoding=texttospeech.AudioEncoding.MP3
                )

                response = self.gcp_tts_client.synthesize_speech(
                    input=input_text, voice=voice, audio_config=audio_config
                )

                self.syntheses.append(response.audio_content)
            except Exception:
                raise
        return self

    def voices(self):
        try:
            return str(self.gcp_tts_client.list_voices())
        except Exception:
            raise
```

## text_to_audio/engines/main.py

```python
from typing import Self
from pydub import AudioSegment
import io


class Engine:
    def __init__(self):
        # The combines synthesis of all parts.
        self.combined_synthesis: bytes = None

        # TTS engines support only a certain amount of chars,
        # so we split the input and store each mp3 bytes in a list.
        self.syntheses: [bytes] = []

    def synthesize(self) -> Self:
        return self

    def voices(self) -> str:
        pass

    def combine_syntheses(self) -> Self:
        self.combined_synthesis = b"".join(self.syntheses)

        combination = None
        for s in self.syntheses:
            sound = AudioSegment.from_mp3(io.BytesIO(s))
            if combination is None:
                combination = sound
            else:
                combination += AudioSegment.silent(duration=1000)
                combination += sound

        self.combined_synthesis = combination.export(format="mp3").read()
        self.syntheses = []
        return self

    def save_as(self, filename: str):
        if self.combined_synthesis is None and len(self.syntheses) > 0:
            self.combine_syntheses()

        with open(filename, "wb") as out:
            out.write(self.combined_synthesis)

    def get_bytes(self) -> bytes:
        if self.combined_synthesis is None and len(self.syntheses) > 0:
            self.combine_syntheses()

        return self.combined_synthesis
```

## text_to_audio/main.py

```python
# from engines import polly, gcp
from fastmcp import FastMCP
from fastmcp.utilities.types import Audio
from engines import polly
from text_to_audio.llm.openai import transcript

mcp = FastMCP("Text to Audio MCP Service")


@mcp.tool
def synthesize(text: str) -> Audio:
    """Synthesize text to speech as an MP3 audio file."""
    # Hard limit of 10k chars for Polly
    # Make separate synthesis tasks for every 2 newlines in a row
    # e.g.
    # <speak>
    #   <s>What up?</s>
    # </speak>
    #
    # <speak> ...
    parts = text[:10000].split("\n\n")

    audio_bytes = polly.PollyClient().synthesize(text_bits=parts).get_bytes()
    return Audio(data=audio_bytes, media_type="audio/mpeg")


@mcp.tool
def generate_transcript(summarized_news: str) -> str:
    """Generate a transcript from summarized news for better Audio synthesis."""
    return transcript(summarized_news)


if __name__ == "__main__":
    mcp.run()
```

## text_to_audio/gen.py

```python
from llm import openai
from engines import polly


def main():
    with open("samples/news-inputs/sample-news-short.txt", "r") as input_file:
        text = input_file.read()

    result = openai.transcript(text)
    with open("gen-result-short.txt", "w") as output_file:
        output_file.write(result)

    parts = result[:10000].split("\n\n")
    polly.PollyClient().synthesize(text_bits=parts).save_as("gen-result-short.mp3")
    print("Done")


if __name__ == "__main__":
    main()
```

## text_to_audio/README.md

````markdown
# Text to Audio

This component will produce an audio file from the given text input.

## Findings

| Product              | Quality | SSLM | Free Quota         | Cost                  |
| -------------------- | ------- | ---- | ------------------ | --------------------- |
| ElevenLabs           | Amazing | No   | 10k chars/mo       | 11$/mo for 100k chars |
| Deepgram             | Good    | No   | 6.6M chars         | $0.03/1k chars        |
| Polly Neural         | Good    | Yes  | 1M chars/mo for 1y | $0.02/1k chars        |
| Google Cloud Chirp 3 | Good    | No   | 1M chars/mo        | $0.03/1k chars        |

## Getting started

Install `uv`: https://docs.astral.sh/uv/#highlights

### To use GCP

Install `gcloud`: https://cloud.google.com/sdk/docs/install

- Login to Google Cloud
- Create a new project
- Search Text to Speech API
- Enable API for the project.
- Open terminal

Authenticate with `gcloud`:

```
gcloud init
```

```
gcloud auth application-default login
```

### To use Polly

1. Copy the example environment file:

```bash
cp .env.example .env
```

2. Edit `.env` and add your AWS credentials:

```bash
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here
AWS_REGION=eu-west-1
```

3. To get your AWS credentials:
   - Login to AWS console via a browser
   - Go to "IAM"
   - Under "Access Management", click "Users"
   - Create user
   - Select from "Permission options": "Attach policies directly"
   - Tick from "Permission policies": AmazonPollyFullAccess
   - Create user
   - Select new user
   - Go to "Security credentials" tab
   - Scroll to "Access keys"
   - Create access key
   - Command Line Interface (CLI)
     - Confirm "I understand above recommendation..."
   - Copy the Access Key ID and Secret Access Key to your `.env` file

### Running the mcp server

```
uv run --env-file .env main.py
```

### Running a test synthesis

```
uv run --env-file .env synth.py
```

## Docker

Building the app:

```bash
DOCKER_BUILDKIT=1 docker build . -t "text-to-audio"
```

### Running the app in Docker

```bash
docker run -it --rm --env-file .env text-to-audio
```

**Note:** The `-it` flag is required because stdio needs standard input session,
otherwise the app will close immediately.
````

## mcp_humorizer/humor.py

```python
from __future__ import annotations

import re
from typing import Literal

from .config import HumorStyle


_WHITESPACE_RE = re.compile(r"\s+")


def _clean(text: str) -> str:
    return _WHITESPACE_RE.sub(" ", (text or "").strip())


def _ensure_sentence(s: str) -> str:
    s = s.strip()
    if not s:
        return s
    if s[-1] in ".!?":
        return s
    return s + "."


def _has_percent(text: str) -> bool:
    return bool(re.search(r"\d+(?:\.\d+)?\s*%", text))


def _has_money(text: str) -> bool:
    return bool(re.search(r"[$€£]\s*\d", text) or re.search(r"\b(?:million|billion|trillion)\b", text, re.I))


def _has_numbers(text: str) -> bool:
    return bool(re.search(r"\d", text))


def _quip_for_numbers(style: HumorStyle) -> str:
    if style in ("sarcastic", "satirical", "roast"):
        return "Relax—my budget is dropping faster than my willpower on pizza night."
    if style == "deadpan":
        return "Comparatively, my savings remain theoretical."
    if style == "absurd":
        return "Meanwhile, the numbers tried to unionize with my calculator."
    if style == "wholesome":
        return "Deep breaths—numbers bounce back, and so can we."
    # light / random / default
    return "On the bright side, my diet is shrinking faster."


def _quip_for_money(style: HumorStyle) -> str:
    if style in ("sarcastic", "satirical"):
        return "Somewhere, a committee just approved a 'vibes only' budget."
    if style == "deadpan":
        return "Fiscal responsibility remains on lunch break."
    if style == "absurd":
        return "A flock of dollar bills migrated south for the winter."
    if style == "wholesome":
        return "Money comes and goes—community and good coffee remain."
    if style == "roast":
        return "Politicians looked at the math and said, 'We prefer interpretive dancing.'"
    return "Wallets are doing cardio; endurance pending."


def _quip_for_generic(style: HumorStyle) -> str:
    if style == "deadpan":
        return "In other news, water is still wet."
    if style == "absurd":
        return "It's like a goose in a board meeting—nobody knows why it's here, but now everyone's honking."
    if style == "wholesome":
        return "Hang in there—every headline has a human on the other side."
    if style in ("satirical", "sarcastic"):
        return "Experts responded by deploying charts, acronyms, and confident nods."
    if style == "roast":
        return "If common sense were Wi‑Fi, this situation would have one bar."
    # light / random / default
    return "So yeah—big mood, tiny attention span, perfect for a short video."


def _tagline(style: HumorStyle) -> str:
    if style == "deadpan":
        return "End of joke. That was the joke."
    if style == "absurd":
        return "Cue the kazoo solo."
    if style == "wholesome":
        return "Stay kind; laugh often."
    if style in ("satirical", "sarcastic"):
        return "Back to you, spin department."
    if style == "roast":
        return "Apply ice to the narrative."
    # light / random / default
    return "Like, follow, and pretend you learned economics."


def humorous_rewrite(summarized_text: str, style: str | HumorStyle = "light") -> str:
    """
    Deterministic humorizer for summarized news text.

    Guarantees:
    - Preserves the original meaning by keeping the first sentence close to the input.
    - Outputs 2–4 short sentences (platform-friendly).
    - Avoids offensive content by using mild, general humor.
    """
    text = _clean(summarized_text)
    if not text:
        return "No input provided. Punchline withheld until further notice."

    # Normalize style
    allowed: tuple[HumorStyle, ...] = (
        "sarcastic",
        "light",
        "absurd",
        "deadpan",
        "wholesome",
        "satirical",
        "roast",
        "random",
    )
    s: HumorStyle = style if style in allowed else "light"
    if s == "random":
        # Deterministic 'random' based on simple hash of content length
        idx = len(text) % 5
        s = ("light", "sarcastic", "deadpan", "absurd", "wholesome")[idx]  # type: ignore[assignment]

    # Line 1: echo core fact concisely (avoid fabricating details)
    line1 = _ensure_sentence(text)

    # Line 2: punchline tailored by content signals
    if _has_money(text):
        line2 = _ensure_sentence(_quip_for_money(s))
    elif _has_percent(text) or _has_numbers(text):
        line2 = _ensure_sentence(_quip_for_numbers(s))
    else:
        line2 = _ensure_sentence(_quip_for_generic(s))

    # Line 3: short analogy / contrast
    if s in ("absurd",):
        line3 = _ensure_sentence("Imagine explaining that to a rubber duck with a briefcase.")
    elif s in ("deadpan",):
        line3 = _ensure_sentence("We remain cautiously unimpressed.")
    elif s in ("satirical", "sarcastic"):
        line3 = _ensure_sentence("Translation: same plot, new press release.")
    elif s == "wholesome":
        line3 = _ensure_sentence("Small steps forward still count.")
    else:
        line3 = _ensure_sentence("Perfect for a 15‑second attention span recap.")

    # Optional Line 4: brief sign-off/tagline
    line4 = _ensure_sentence(_tagline(s))

    # Keep to 3–4 sentences depending on input length
    sentences = [line1, line2, line3]
    if len(text) > 80:
        sentences.append(line4)

    # Join and ensure compact output
    out = " ".join(sentences)
    return _clean(out)
```

## mcp_humorizer/mcp_server.py

```python
from __future__ import annotations

import sys
import logging

try:
    # FastMCP is the ergonomic Python helper for building MCP servers
    from mcp.server.fastmcp import FastMCP  # type: ignore
except Exception as e:  # pragma: no cover
    print("ERROR: Missing or incompatible 'mcp' Python package. Please install with:")
    print("  pip install -r mcp_humorizer/requirements.txt")
    print(f"Details: {e}")
    sys.exit(1)

from .config import Settings
from .engine import comedicize_text

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

app = FastMCP("mcp-humorizer")


@app.tool()
def comedicize(id: str, summarized_text: str) -> dict:
    """
    Transform summarized news text into comedic text.

    API Contract:
    Input:
    {
      "id": "uuid",
      "summarized_text": "The economy shrank by 2% last quarter."
    }

    Output:
    {
      "id": "uuid",
      "comedic_text": "The economy shrank by 2%. Don’t worry, my diet is shrinking faster!"
    }
    """
    settings = Settings.from_env()
    result = comedicize_text(summarized_text, settings)
    return {"id": id, "comedic_text": result}


@app.tool()
def health() -> dict:
    """
    Simple health check tool to verify server connectivity.
    """
    settings = Settings.from_env()
    return {
        "name": "mcp-humorizer",
        "provider": settings.model_provider,
        "humor_style": settings.humor_style,
        "status": "ok",
    }


def main() -> None:
    # Runs an MCP server over stdio
    app.run()


if __name__ == "__main__":
    main()
```

## mcp_humorizer/tests/test_humor.py

```python
from __future__ import annotations

import re

from mcp_humorizer.humor import humorous_rewrite


def _count_sentences(text: str) -> int:
    # Count sentences by splitting on ., !, ? and filtering empties
    parts = re.split(r"[.!?]+", text)
    return sum(1 for p in parts if p.strip())


def test_humor_preserves_fact_and_is_punchy():
    summary = "The economy shrank by 2% last quarter"
    out = humorous_rewrite(summary, style="light")

    # Fact should be present verbatim (first sentence echoes the input with punctuation)
    assert summary in out

    # Output should be short-form (2–4 sentences)
    n = _count_sentences(out)
    assert 2 <= n <= 4

    # Number/percent punchline should appear for numeric inputs (light style default)
    assert "shrinking faster" in out.lower()


def test_humor_money_trigger_light_style():
    summary = "The budget was cut by $5 million for next year"
    out = humorous_rewrite(summary, style="light")

    # Fact preserved
    assert summary in out

    # Money quip for default/light style
    assert "Wallets are doing cardio" in out


def test_humor_generic_trigger_deadpan():
    summary = "Scientists discovered a new exoplanet nearby"
    out = humorous_rewrite(summary, style="deadpan")

    # Fact preserved
    assert summary in out

    # Deadpan specific lines likely present
    assert "In other news, water is still wet." in out or "We remain cautiously unimpressed." in out

    # Sentence count still constrained
    n = _count_sentences(out)
    assert 2 <= n <= 4
```

## mcp_humorizer/__init__.py

```python
"""
mcp-humorizer

MCP Server – Summarized Text → Comedic Text
- Receives summarized news text and transforms it into comedic text.
- Pluggable LLM providers (OpenAI, Anthropic) with an offline deterministic fallback.
- Exposes MCP tools via FastMCP:
    - comedicize(id: str, summarized_text: str) -> {"id": str, "comedic_text": str}
    - health() -> basic server status info
"""

from __future__ import annotations

__all__ = [
    "__version__",
    "comedicize_text",
    "Settings",
    "HumorStyle",
    "Provider",
    "build_system_prompt",
]

__version__ = "0.1.0"

from .engine import comedicize_text
from .config import Settings, HumorStyle, Provider, build_system_prompt
```

## mcp_humorizer/mcp-humorizer.md

````markdown
# MCP Server – Summarized Text → Comedic Text

## Overview

This MCP server receives **summarized news text** and transforms it into **comedic text**, maintaining the original meaning while injecting humor, satire, and punchlines. It acts as the **Humor Engine** of *Skibidi News*, bridging factual news with digestible entertainment.

## Responsibilities

1. **Input:** Summarized text (from News Aggregation service).
2. **Processing:**

   * Parse and analyze text context.
   * Apply comedic prompt templates and humor strategies.
   * Optimize for short, punchy delivery (suited for TikTok/short-form content).
3. **Output:** Comedic text (ready for transcript → audio or transcript → video stages).

## API Contract

* **Input:**

  ```json
  {
    "id": "uuid",
    "summarized_text": "The economy shrank by 2% last quarter."
  }
  ```

* **Output:**

  ```json
  {
    "id": "uuid",
    "comedic_text": "The economy shrank by 2%. Don’t worry, my diet is shrinking faster!"
  }
  ```

## Connection with Router Agent

* Registered as an MCP Server (`mcp-humorizer`).
* Communicates with Router Agent to receive jobs and return results.
* AI model (LLM) can be swapped easily thanks to containerization.

## Deployment

* **Containerized (Docker):** Enables independent deployment and scalability.
* **Configurable AI backend:** OpenAI, Anthropic, or custom humor-tuned models.
* **Environment variables:**
  * `MODEL_PROVIDER`
  * `API_KEY`
  * `HUMOR_STYLE` (sarcastic, light, absurd, etc.)

## Example Flow

1. Router Agent receives summarized text.
2. Passes payload to MCP server.
3. Server applies humor transformation.
4. Comedic text is returned to Router Agent.

## Notes

* Humor should be **short-form, punchy, and platform-ready**.
* Ensure output remains tied to actual summarized content (not fake news).
* Comedic “flavor” can be adjusted via system prompt or config.
````

## Statistics

- Total Files: 31
- Total Characters: 113717
- Total Tokens: 0
